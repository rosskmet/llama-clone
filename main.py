# Import necessary libraries
import torch
from torch import nn
from torch.nn import functional as F

import numpy as np
import time
from dataclasses import dataclass
import pandas as pd
from matplotlib import pyplot as plt

from model import ModelArgs
import input
from decoder import Attention, FeedForward, RMSNorm, RoPE, TransformerBlock, repeat_kv
from output import Transformer


# Define function to generate batches from the given dataset
def get_dataset_batch(ib, data, split, args:ModelArgs):
  seq_len = args.max_seq_len
  batch_size = args.max_batch_size
  device = args.device

  train = data[:int(0.8 * len(data))]
  val = data[int(0.8 * len(data)): int(0.9 * len(data))]
  test = data[int(0.9 * len(data)):]

  batch_data = train
  if split == "val":
    batch_data = val

  if split == "test":
    batch_data = test
  
  # Picking random starting points from the dataset to give random samples for training, validation and testing.
  
  ix = torch.randint(0, len(batch_data) - seq_len - 3, (batch_size,)).to(device)
  x = torch.stack([torch.cat([ib.token_bos, batch_data[i:i+seq_len-1]]) for i in ix]).long().to(device)
  y = torch.stack([torch.cat([batch_data[i+1:i+seq_len], ib.token_eos]) for i in ix]).long().to(device)
  
  return x,y

# Define a evaluate loss function to calculate and store training and validation loss for logging and plotting
@torch.no_grad()
def evaluate_loss(ib, model, dataset, args:ModelArgs):
  out = {}
  model.eval()

  for split in ["train", "val"]:
    losses = []
    for _ in range(10):      
      xb, yb = get_dataset_batch(ib, dataset, split, args)
      _, loss = model(x=xb, targets=yb)
      losses.append(loss.item())
    out[split] = np.mean(losses)

  model.train()
  return out

# Define a training function to perform model training
def train(ib, model, dataset, optimizer, args:ModelArgs):
    epochs = args.epochs
    log_interval = args.log_interval
    device = args.device
    losses = []   
    start_time = time.time()

    for epoch in range(epochs):
        optimizer.zero_grad()
        
        xs, ys = get_dataset_batch(ib, dataset, 'train', args)
        xs = xs.to(device)
        ys = ys.to(device)
        logits, loss = model(x=xs, targets=ys)
        loss.backward()
        optimizer.step()

        if epoch % log_interval == 0:
            batch_time = time.time() - start_time
            x = evaluate_loss(ib, model, dataset, args)
            losses += [x]            
            print(f"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f}")
            start_time = time.time()
    
    # Print the final validation loss
    print("validation loss: ", losses[-1]['val'])
    # Display the interval losses in plot 
    return pd.DataFrame(losses).plot()

def sample_top_p(probs, p):
    probs_sort, prob_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort[mask] = 0.0
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = torch.multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(prob_idx, -1, next_token)    
    # Sampled token indices from the vocabular is returned 
    return next_token

## Step 5: Inference Llama 3 Model:
# This function generates text sequences based on provided prompts using the LLama 3 model we've built and trained.

def generate(ib, model, prompts: str, params: ModelArgs, max_gen_len: int=500, 
             temperature: float = 0.6, top_p: float = 0.9, ):

    # prompt_tokens: List of user input texts or prompts
    # max_gen_len: Maximum length of the generated text sequence.
    # temperature: Temperature value for controlling randomness in sampling. Defaults to 0.6.
    # top_p: Top-p probability threshold for sampling prob output from the logits. Defaults to 0.9.
    # prompt_tokens = [0]
    bsz = 1  #For inferencing, in general user just input one prompt which we'll take it as 1-batch
    prompt_tokens = ib.token_bos.tolist() + ib.encode(prompts)
    assert len(prompt_tokens) <= params.max_seq_len, "prompt token length should be small than max_seq_len"
    total_len = min(len(prompt_tokens)+max_gen_len, params.max_seq_len)   

    # this tokens matrix is to store the input prompts and all the output that is generated by model.
    # later we'll use the tokenizers decode function to decode this token to view results in text format
    tokens = torch.full((bsz,total_len), fill_value=ib.token_pad.item(), dtype=torch.long, device=params.device)

    # fill in the prompt tokens into the token matrix
    tokens[:,:len(prompt_tokens)] = torch.tensor(prompt_tokens, dtype=torch.long, device=params.device)

    #create a prompt_mask_token for later use to identify if the token is a prompt token or a padding token
    # True if it is a prompt token, False if it is a padding token
    input_text_mask = tokens != ib.token_pad.item()

    #now we can start inferencing using one token at a time from the prompt_tokens list starting with the first position.
    prev_pos = 0
    for cur_pos in range(1, total_len):
      with torch.no_grad():
        logits, _ = model(x=tokens[:,prev_pos:cur_pos], start_pos=prev_pos)
      if temperature > 0:      
        probs = torch.softmax(logits[:, -1]/temperature, dim=-1)
        next_token = sample_top_p(probs, top_p)        
      else:
        next_token = torch.argmax(logits[:, -1], dim=-1)        

      next_token = next_token.reshape(-1)

      # only replace the token if it's a padding token
      next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)
      tokens[:, cur_pos] = next_token

      prev_pos = cur_pos
      if tokens[:,cur_pos]==ib.token_pad.item() and next_token == ib.token_eos.item():
        break

    output_tokens, output_texts = [], []    

    for i, toks in enumerate(tokens.tolist()):
      # eos_idx = toks.index(token_eos.item())
      if ib.token_eos.item() in toks:
        eos_idx = toks.index(ib.token_eos.item())
        toks = toks[:eos_idx]

      output_tokens.append(toks)
      output_texts.append(ib.decode(toks))
    return output_tokens, output_texts


def main():
    # prompts = "Hello World"
    prompts = "Consider you what services he has done"
    ib = input.InputBlock(prompts)
    vocab_size = len(ib.vocab)

    x = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=ModelArgs.device)
    rms_norm = RMSNorm(dim=ModelArgs.dim)
    x_norm = rms_norm(x)

    head_dim = ModelArgs.dim//ModelArgs.n_heads
    wq = nn.Linear(ModelArgs.dim, ModelArgs.n_heads * head_dim, bias=False, device=ModelArgs.device)
    wk = nn.Linear(ModelArgs.dim, ModelArgs.n_kv_heads * head_dim, bias=False, device=ModelArgs.device)
    xq = wq(x_norm)
    xk = wk(x_norm)
    print(f"xq.shape: {xq.shape}")
    print(f"xk.shape: {xk.shape}")

    xq = xq.view(xq.shape[0],xq.shape[1],ModelArgs.n_heads, head_dim)
    xk = xk.view(xk.shape[0],xk.shape[1],ModelArgs.n_kv_heads, head_dim)
    print(f"xq.re-shape: {xq.shape}")
    print(f"xk.re-shape: {xk.shape}")

    rope = RoPE()
    freqs_cis = rope.precompute_freqs_cis(dim=head_dim, seq_len=ModelArgs.max_seq_len)
    print(f"freqs_cis.shape: {freqs_cis.shape}")

    xq_rotate, xk_rotate = rope.apply_rotary_emb(xq, xk, freqs_cis)
    print(f"xq_rotate.shape: {xq_rotate.shape}")
    print(f"xk_rotate.shape: {xk_rotate.shape}")

    n_rep = ModelArgs.n_heads // ModelArgs.n_kv_heads
    keys = repeat_kv(xk, n_rep)
    print(f"xk.shape: {xk.shape}")
    print(f"keys.shape: {keys.shape}")

    attention = Attention(ModelArgs)
    x_out = attention(x_norm, start_pos=0, inference=False)
    print(f"x_out.shape: {x_out.shape}")

    feed_forward = FeedForward(ModelArgs.dim, 4 * ModelArgs.dim, ModelArgs.multiple_of, ModelArgs.ffn_dim_multiplier)
    x_out = rms_norm(x_out)
    x_out = feed_forward(x_out)
    print(f"feed forward output: x_out.shape: {x_out.shape}")

    transformer_block = TransformerBlock(ModelArgs)
    transformer_block_out = transformer_block(x, start_pos=0, inference=False)
    print(f"transformer_block_out.shape: {transformer_block_out.shape}")

    dataset = torch.tensor(ib.encode(ib.data), dtype=torch.int).to(ModelArgs.device)
    print(f"dataset-shape: {dataset.shape}")

    model = Transformer(ModelArgs, vocab_size).to(ModelArgs.device)
    optimizer = torch.optim.Adam(model.parameters())

    # How do I save the model to save compute, maybe finetune?
    train(ib, model, dataset, optimizer, ModelArgs)

    output_tokens, output_texts = generate(ib, model, prompts, ModelArgs)
    output_texts = output_texts[0].replace("<|begin_of_text|>", "")
    print(output_texts)


if __name__ == '__main__':

    main()